{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/ner_dataset.csv',encoding='latin1')\n",
    "data.fillna(method=\"ffill\",inplace=True)\n",
    "\n",
    "max_len = 75\n",
    "max_len_char = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tag'].value_counts()\n",
    "include_tags = [\n",
    "    'B-geo',\n",
    "    'I-geo',\n",
    "    'B-tim',\n",
    "    'I-tim',\n",
    "    \n",
    "]\n",
    "for tag in data['Tag'].unique():\n",
    "    if tag not in include_tags:\n",
    "        data.loc[data['Tag']==tag,'Tag'] = 'O'\n",
    "        \n",
    "n_tags = data['Tag'].unique().shape[0]\n",
    "\n",
    "tag_mapping = dict()\n",
    "for tag_num,tag in enumerate(data['Tag'].unique().tolist()):\n",
    "    tag_mapping[tag] = tag_num\n",
    "data['Tag_num'] = data['Tag'].map(tag_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tag_num'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words)\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func_words = lambda s: [w for w in s[\"Word\"].values.tolist()]\n",
    "X = data.groupby(\"Sentence #\").apply(agg_func_words)\n",
    "\n",
    "agg_func_labels = lambda s: [w for w in s[\"Tag_num\"].values.tolist()]\n",
    "y = data.groupby(\"Sentence #\").apply(agg_func_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "X_word = [[word2idx[w] for w in s] for s in X.tolist()]\n",
    "X_word = pad_sequences(maxlen=max_len, sequences=X_word, value=word2idx[\"PAD\"], padding='post', truncating='post')\n",
    "\n",
    "\n",
    "chars = set([w_i for w in words for w_i in w])\n",
    "n_chars = len(chars)\n",
    "print(n_chars)\n",
    "\n",
    "char2idx = {c: i + 2 for i, c in enumerate(chars)}\n",
    "char2idx[\"UNK\"] = 1\n",
    "char2idx[\"PAD\"] = 0\n",
    "X_char = []\n",
    "for sentence in X.tolist():\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"PAD\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    X_char.append(np.array(sent_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #\n",
       "Sentence: 1        [Thousands, of, demonstrators, have, marched, ...\n",
       "Sentence: 10       [Iranian, officials, say, they, expect, to, ge...\n",
       "Sentence: 100      [Helicopter, gunships, Saturday, pounded, mili...\n",
       "Sentence: 1000     [They, left, after, a, tense, hour-long, stand...\n",
       "Sentence: 10000    [U.N., relief, coordinator, Jan, Egeland, said...\n",
       "Sentence: 10001    [Mr., Egeland, said, the, latest, figures, sho...\n",
       "Sentence: 10002    [He, said, last, week, 's, tsunami, and, the, ...\n",
       "Sentence: 10003        [Some, 1,27,000, people, are, known, dead, .]\n",
       "Sentence: 10004    [Aid, is, being, rushed, to, the, region, ,, b...\n",
       "Sentence: 10005    [Lebanese, politicians, are, condemning, Frida...\n",
       "Sentence: 10006    [In, Beirut, ,, a, string, of, officials, voic...\n",
       "Sentence: 10007    [One, person, was, killed, and, more, than, 20...\n",
       "Sentence: 10008    [Lebanon, has, suffered, a, series, of, bombin...\n",
       "Sentence: 10009    [Syria, is, widely, accused, of, involvement, ...\n",
       "Sentence: 1001     [The, global, financial, crisis, has, left, Ic...\n",
       "Sentence: 10010    [Israeli, officials, say, Prime, Minister, Ari...\n",
       "Sentence: 10011    [Doctors, describe, the, tiny, hole, as, a, mi...\n",
       "Sentence: 10012    [The, procedure, ,, known, as, cardiac, cathet...\n",
       "Sentence: 10013    [Doctors, say, they, expect, Mr., Sharon, will...\n",
       "Sentence: 10014    [Mr., Sharon, returned, to, work, on, December...\n",
       "Sentence: 10015    [Doctors, say, the, stroke, has, not, caused, ...\n",
       "Sentence: 10016    [The, designers, of, the, first, private, mann...\n",
       "Sentence: 10017    [SpaceShipOne, designer, Burt, Rutan, accepted...\n",
       "Sentence: 10018    [To, win, the, money, ,, SpaceShipOne, had, to...\n",
       "Sentence: 10019    [The, spacecraft, made, its, flights, in, late...\n",
       "Sentence: 1002     [Three, major, banks, have, collapsed, ,, unem...\n",
       "Sentence: 10020    [The, vehicle, had, to, carry, a, pilot, and, ...\n",
       "Sentence: 10021    [SpaceShipOne, was, financed, with, more, than...\n",
       "Sentence: 10022    [North, Korea, says, flooding, caused, by, las...\n",
       "Sentence: 10023    [The, state, news, agency, KCNA, reported, the...\n",
       "                                         ...                        \n",
       "Sentence: 9972     [Ice, Cube, 's, family, comedy, Are, We, Done,...\n",
       "Sentence: 9973     [Quentin, Tarantino, and, Robert, Rodriguez, '...\n",
       "Sentence: 9974     [Media, tracker, Paul, Dergarabedian, says, th...\n",
       "Sentence: 9975     [In, Afghanistan, ,, a, huge, explosion, in, a...\n",
       "Sentence: 9976     [Afghan, officials, say, the, explosion, in, t...\n",
       "Sentence: 9977     [They, say, a, third, German, soldier, and, a,...\n",
       "Sentence: 9978     [The, Associated, Press, quotes, Afghan, Presi...\n",
       "Sentence: 9979     [The, German, soldiers, were, part, of, the, N...\n",
       "Sentence: 998      [Thousands, of, Icelanders, marked, the, 90th,...\n",
       "Sentence: 9980     [The, German, detachment, of, about, two, thou...\n",
       "Sentence: 9981     [ISAF, has, about, 8,000, troops, in, Afghanis...\n",
       "Sentence: 9982     [Pakistani, police, reported, more, deaths, Sa...\n",
       "Sentence: 9983     [Several, people, are, still, being, treated, ...\n",
       "Sentence: 9984     [Liquor, is, banned, for, Muslims, in, Pakista...\n",
       "Sentence: 9985     [But, some, Muslims, drink, alcohol, ,, resort...\n",
       "Sentence: 9986     [Police, say, they, have, arrested, several, m...\n",
       "Sentence: 9987     [In, October, of, last, year, ,, at, least, 12...\n",
       "Sentence: 9988     [A, former, U.S., Senate, majority, leader, sa...\n",
       "Sentence: 9989     [Democrat, Tom, Daschle, contradicts, Presiden...\n",
       "Sentence: 999      [Hundreds, of, marchers, tried, to, storm, cen...\n",
       "Sentence: 9990     [In, an, opinion, piece, in, the, Washington, ...\n",
       "Sentence: 9991     [President, Bush, last, week, confirmed, he, s...\n",
       "Sentence: 9992     [He, called, it, a, vital, tool, for, national...\n",
       "Sentence: 9993     [Iran, 's, elite, security, forces, are, warni...\n",
       "Sentence: 9994     [Opposition, activists, have, called, for, pro...\n",
       "Sentence: 9995     [Opposition, leader, Mir, Hossein, Mousavi, ha...\n",
       "Sentence: 9996     [On, Thursday, ,, Iranian, state, media, publi...\n",
       "Sentence: 9997     [Following, Iran, 's, disputed, June, 12, elec...\n",
       "Sentence: 9998     [Since, then, ,, authorities, have, held, publ...\n",
       "Sentence: 9999     [The, United, Nations, is, praising, the, use,...\n",
       "Length: 47959, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        0.931413\n",
       "B-geo    0.035900\n",
       "B-tim    0.019391\n",
       "I-geo    0.007071\n",
       "I-tim    0.006226\n",
       "Name: Tag, dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tag'].value_counts()/data['Tag'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras_wc_embd import WordCharEmbd,get_word_list_eng\n",
    "\n",
    "def get_wordchar_embedding(data):\n",
    "    '''\n",
    "    This function takes the tokenized reports and transforms them to both\n",
    "    a word and character embedding with this handy library:\n",
    "    https://github.com/CyberZHG/keras-word-char-embd\n",
    "\n",
    "    input: data, the dataframe that contains the 'reports' with the preprocessed reports\n",
    "    output: wc_embd, a class that stores dictionaries of word embeddings, character embeddings,\n",
    "    and methods for creating and updating the word and character embeddings\n",
    "    '''\n",
    "    wc_embd = WordCharEmbd(\n",
    "        word_min_freq=5,\n",
    "        char_min_freq=2,\n",
    "        word_ignore_case=True,\n",
    "        char_ignore_case=True,\n",
    "    )\n",
    "    [wc_embd.update_dicts(report) for report in data]\n",
    "    return wc_embd\n",
    "\n",
    "wc_embd = get_wordchar_embedding(X.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_emb, embd_layer = wc_embd.get_embedding_layer(word_mask_zero=False,char_mask_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Input_Word_14:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'Input_Char_14:0' shape=(?, ?, 21) dtype=float32>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The shape of the input to \"Flatten\" is not fully defined (got (None, 128). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-9705b53ff722>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_cnnbilstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-9705b53ff722>\u001b[0m in \u001b[0;36mcreate_cnnbilstm_model\u001b[0;34m(wc_embd)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mlstm_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#lstm_model = Model(inputs=inputs_emb, outputs=lstm_layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mflatten_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0msoftmax_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_tags\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/miami_datascience_meetup/miami/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m             if all([s is not None\n\u001b[1;32m    473\u001b[0m                     for s in to_list(input_shape)]):\n\u001b[0;32m--> 474\u001b[0;31m                 \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/miami_datascience_meetup/miami/lib/python3.7/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    498\u001b[0m             raise ValueError('The shape of the input to \"Flatten\" '\n\u001b[1;32m    499\u001b[0m                              \u001b[0;34m'is not fully defined '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m                              \u001b[0;34m'(got '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m                              \u001b[0;34m'Make sure to pass a complete \"input_shape\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                              \u001b[0;34m'or \"batch_input_shape\" argument to the first '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The shape of the input to \"Flatten\" is not fully defined (got (None, 128). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model."
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional,LSTM,Conv1D,Input,Dense,Dropout,concatenate,MaxPooling1D,TimeDistributed,Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "def create_cnnbilstm_model(wc_embd):\n",
    "    '''\n",
    "    This function assembles the word, character embedding and the keras Bi-LSTM for classification\n",
    "    input: wc_embed, the word+character embedding layers of the network\n",
    "    output: compiled, untrained keras model\n",
    "\n",
    "    The network has two inputs layers:\n",
    "    1) The word+char embedding layer that takes the embedded report date\n",
    "    2) The body part encoding input that indicates whether the text is about knee or shoulder\n",
    "\n",
    "    These two inputs are concatenated at the final dense perceptron layer\n",
    "    '''\n",
    "    inputs_emb, embd_layer = wc_embd.get_embedding_layer(word_mask_zero=False,char_mask_zero=False)\n",
    "\n",
    "    drop_out = Dropout(0.5)(embd_layer)\n",
    "    lstm_layer = Bidirectional(LSTM(units=64, name='LSTM', dropout=0.3,return_sequences=True))(drop_out)#32\n",
    "    #lstm_model = Model(inputs=inputs_emb, outputs=lstm_layer)\n",
    "    flatten_layer = Flatten()(lstm_layer)\n",
    "\n",
    "    softmax_layer = TimeDistributed(Dense(units=n_tags+1, activation='softmax', name='Softmax'))(flatten_layer)\n",
    "    model = Model(inputs=inputs_emb, outputs=softmax_layer)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = create_cnnbilstm_model(wc_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected time_distributed_4 to have 3 dimensions, but got array with shape (47959, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-76696875b9ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/Documents/miami_datascience_meetup/miami/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/miami_datascience_meetup/miami/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/miami_datascience_meetup/miami/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected time_distributed_4 to have 3 dimensions, but got array with shape (47959, 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "history = model.fit(\n",
    "    wc_embd.get_batch_input(X.tolist()),\n",
    "    np.array(y.tolist()),\n",
    "    epochs=20,\n",
    "    batch_size = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
